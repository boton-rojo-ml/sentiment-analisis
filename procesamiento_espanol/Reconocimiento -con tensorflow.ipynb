{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de comentarios negativos\n",
    "\n",
    "En esta libreta vamos a utilizar una red neuronal profunda para tratar de detectar los comentarios negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Para el tratamiento del lenguaje\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import sklearn.metrics as metricas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Recuperar y tratar los datos para usarse\n",
    "\n",
    "Los datos se encuentran en un directorio privado, los cuales ya se \n",
    "encuentran separados de un conjunto de prueba. \n",
    "\n",
    "Los datos se encuentran como una tabla tipo csv de la siguiente manera:\n",
    "\n",
    "Columnas  |    Nombre       |     Descripción\n",
    "----------|-----------------|----------------------------------------------------------\n",
    "1         |   id            |    Identificador del mensaje\n",
    "2         |   create_time   |    Fecha de creación del mensaje\n",
    "3         |   id_from       |    Identificador del usuario (emisor del mensaje)\n",
    "4         |   comment       |    El mensaje en si mismo\n",
    "5         |   like_count    |    Número de likes que recibió el mensaje\n",
    "6         |   flag          |    Polaridad (0 -> Negativo, 1 -> Positivo, 2 -> Neutro)\n",
    "7         |   ready         |    Revisado por un operador (1 -> No, 2 -> Si)\n",
    "\n",
    "Una vez recuperados los datos, se realiza el siguiente pretratamiento:\n",
    "\n",
    "1. La columna `flag` será el valor de salida y la columna `comment` \n",
    "   como variables de entrada, las cuales son string. \n",
    "   \n",
    "2. La columna `comment`la vamos a tratar, ya que cada usuario maneja \n",
    "   codificaciones diferentes y los datos están hechos un chile con queso.\n",
    "\n",
    "3. La columna `flag` se le asigna valor de 0 a los valores 1 y 2 \n",
    "   (comentarios no negativos) y valor de 1 a los que tienen valor de 0\n",
    "   (comentarios negativos). Convertimos el problema a un problema de\n",
    "   clasificación binaria.\n",
    "   \n",
    "4. Los datos se separan, 80% para entrenamiento y 20% para validación, \n",
    "   procurando que en ambos conjuntos haya la misma proporción de \n",
    "   elementos de ambas clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "df = pd.read_csv(\"privado/data_train.csv\")\n",
    "df = df.loc[df['ready'] == 2]\n",
    "df.index = range(len(df))\n",
    "\n",
    "X = df[\"comment\"]\n",
    "X = X.apply(lambda x: str(x).decode('unicode_escape').encode('utf-8', 'ignore').strip())\n",
    "\n",
    "y = np.array(df['flag'])\n",
    "\n",
    "# Problema binario 1 = Comentario negativo\n",
    "y = np.where(y > 0, 0, 1)\n",
    "\n",
    "# Separamos 20% para validación\n",
    "indices = StratifiedKFold(y, n_folds=5, shuffle=True, random_state=0)\n",
    "for tr_i, va_i in indices:\n",
    "    X_train, X_valid = X.loc[tr_i], X.loc[va_i]\n",
    "    y_train, y_valid = y[tr_i], y[va_i]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Genera la serie de operaciones a realizar.\n",
    "\n",
    "En esta paso vamos a decidir cuales métodos vamos a utilizar para tratar la información,\n",
    "y cual método vamos a utilizar para la clasificación. Los métodos que se seleccionen serán\n",
    "incluídos en un `pipeline` que nos ayuda a serializar las operaciones a realizar.\n",
    "\n",
    "Entre los métodos de vectorización de la información que podemos utilizar se encuentran:\n",
    "\n",
    "1. Bolsa de palabras (o bigramas): `CountVectorizer`\n",
    "2. Tf–idf term weighting: `TfidfVectorizer`\n",
    "\n",
    "Entre los métodos de reducción de la dimensionalidad se encuentran:\n",
    "\n",
    "1. Análisis en componentes principales para datos dispersos: `SparsePCA`\n",
    "2. Latent semantic analysis (LSA): `TruncatedSVD`\n",
    "\n",
    "Estos pueden ser utilizados en forma combinada. Los métodos de clasificación pueden ser:\n",
    "\n",
    "1. Máquina de Vectores de Soporte Lineal: `LinearSVC`\n",
    "2. Máquina de Vectores de Soporte: `SVC`\n",
    "3. Naive Bayes: `MultinomialNB`\n",
    "4. Bosques aleatorios: `RandomForestClassifier`\n",
    "5. AdaBoost: `AdaBoostClassifier`\n",
    "6. Árbol de desición: `DecisionTreeClassifier`\n",
    "\n",
    "Por último, es necesario decidir si hay parámetros que pueden ser modificados, y en que términos pueden ser modificados, esto se agrega en el diccionario `parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "espanol_stopwords = stopwords.words('spanish')\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    lowercase=True,\n",
    "    max_df = 1.0,\n",
    "    min_df = 0.0005,\n",
    "    ngram_range=(2, 2),\n",
    "    max_features=40000,\n",
    "    stop_words=espanol_stopwords)\n",
    "\n",
    "reductor = TruncatedSVD(n_components=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cls = tf.contrib.learn.TensorFlowDNNClassifier(\n",
    "    hidden_units=[20, 100],\n",
    "    n_classes=2,\n",
    "    optimizer=\"Adam\",\n",
    "    class_weight=[.87, 0.6],\n",
    "    steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3. Ejecutar preprocesamiento\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_vec = vectorizer.fit_transform(X_train)\n",
    "X_red = reductor.fit_transform(X_vec)\n",
    "#X_red = X_vec.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso: 4 Ejecutar aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #99, avg. train loss: 0.36661\n",
      "Step #199, avg. train loss: 0.35403\n",
      "Step #299, avg. train loss: 0.38048\n",
      "Step #399, avg. train loss: 0.35710\n",
      "Step #499, avg. train loss: 0.35410\n",
      "Step #599, avg. train loss: 0.36157\n",
      "Step #699, avg. train loss: 0.34340\n",
      "Step #799, avg. train loss: 0.34924\n",
      "Step #899, avg. train loss: 0.34359\n",
      "Step #999, avg. train loss: 0.34460\n",
      "Step #1099, avg. train loss: 0.33306\n",
      "Step #1199, avg. train loss: 0.32472\n",
      "Step #1300, epoch #1, avg. train loss: 0.32353\n",
      "Step #1400, epoch #1, avg. train loss: 0.34540\n",
      "Step #1500, epoch #1, avg. train loss: 0.33732\n",
      "Step #1600, epoch #1, avg. train loss: 0.35395\n",
      "Step #1700, epoch #1, avg. train loss: 0.32728\n",
      "Step #1800, epoch #1, avg. train loss: 0.34226\n",
      "Step #1900, epoch #1, avg. train loss: 0.33147\n",
      "Step #2000, epoch #1, avg. train loss: 0.33862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorFlowDNNClassifier(batch_size=32, class_weight=[0.87, 0.6],\n",
       "            clip_gradients=5.0, config=None, continue_training=False,\n",
       "            dropout=None, hidden_units=[20, 100], learning_rate=0.1,\n",
       "            n_classes=2, optimizer='Adam', steps=2000, verbose=1)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.fit(X=X_red, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4: Analisis de los resultados\n",
    "\n",
    "Por último analizamos que tan bien se desempeña en los datos de validación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8542    0]\n",
      " [1184    0]]\n"
     ]
    }
   ],
   "source": [
    "# Muestra los resultados con datos de validacion\n",
    "X_vec = vectorizer.transform(X_valid)\n",
    "X_red = reductor.transform(X_vec)\n",
    "#X_red = X_vec.toarray()\n",
    "y_est = cls.predict(X_red)\n",
    "print metricas.confusion_matrix(y_valid, y_est)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33128  1037]\n",
      " [ 2295  2439]]\n"
     ]
    }
   ],
   "source": [
    "X_vec = vectorizer.transform(X_train)\n",
    "X_red = reductor.transform(X_vec)\n",
    "y_est = cls.predict(X_red)\n",
    "print metricas.confusion_matrix(y_train, y_est)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
